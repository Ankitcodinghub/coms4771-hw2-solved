# coms4771-hw2-solved
**TO GET THIS SOLUTION VISIT:** [COMS4771 HW2 Solved](https://www.ankitcodinghub.com/product/coms4771-hw2-solved/)


---

üì© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
üì± **WhatsApp:** +1 419 877 7882  
üìÑ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;52563&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;1&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;5&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;5\/5 - (1 vote)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;COMS4771 HW2 Solved&quot;,&quot;width&quot;:&quot;138&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 138px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            5/5 - (1 vote)    </div>
    </div>
<ul>
<li>[Constrained optimization] Show that the distance from the hyperplane <em>g</em>(<em>x</em>) = <em>w</em><em>x</em>+<em>w</em><sub>0 </sub>= 0 to a point <em>x<sub>a </sub></em>is |<em>g</em>(<em>x<sub>a</sub></em>)|<em>/</em>k<em>w</em>k by minimizing the squared distance k<em>x </em>‚àí <em>x<sub>a</sub></em>k<sup>2 </sup>subject to the constraint <em>g</em>(<em>x</em>) = 0.</li>
<li>[A better output Perceptron algorithm guarantee] In class, we saw that when the training sample <em>S </em>is linearly separable with a maximum margin <em>Œ≥ &gt; </em>0, then the Perceptron algorithm run cyclically over <em>S </em>is guaranteed to converge after updates, where <em>R </em>is the radius of the sphere containing the sample points. This does not guarantee however that the hyperplane solution returned by Perceptron, i.e. <em>w<sub>T </sub></em>achieves a margin close to <em>Œ≥</em>.
<ul>
<li>Show an example training dataset <em>S </em>in R<sup>2 </sup>that has margin <em>Œ≥</em>, and an order of updates made by the Perceptron algorithm where the hyperplane solution returned has arbitrarily bad margin on <em>S</em>.</li>
<li>Consider the following modification to the perceptron algorithm:</li>
</ul>
</li>
</ul>
Modified Perceptron Algorithm

<em>Input: training dataset </em><em>S </em>= (<em>x<sub>i</sub>,y<sub>i</sub></em>)<em><sub>i</sub></em><sub>=1<em>,‚Ä¶,n</em></sub>

<em>Output: learned vector </em><em>w</em>

<ul>
<li>Initialize <em>w</em><sub>0 </sub>:= 0<em>,t </em>:= 0</li>
<li>while there exists an example (<em>x,y</em>) ‚àà <em>S</em>, such that 2<em>y</em>(<em>w<sub>t </sub></em> <em>x</em>) ‚â§ <em>Œ≥</em>k<em>w<sub>t</sub></em>k</li>
<li>set <em>w<sub>t</sub></em><sub>+1 </sub>:= <em>w<sub>t </sub></em>+ <em>yx</em></li>
<li>set <em>t </em>:= <em>t </em>+ 1</li>
<li>return <em>w<sub>t</sub></em>.</li>
<li>If the Modified Perceptron Algorithm (MPA) terminates after <em>T </em>rounds, what margin guarantee is achieved by the hyperplane <em>w<sub>T </sub></em>returned by MPA? Justify your answer.</li>
<li>We will now prove step-by-step the mistake bound for the Modified Perceptron Algorithm (MPA) algorithm.
<ol start="2">
<li>Show that after <em>T </em>rounds <em>TŒ≥ </em>‚â§ k<em>w<sub>T</sub></em>k, and observe that if k<em>w<sub>T</sub></em>k <em>&lt; </em>4<em>R</em><sup>2</sup><em>/Œ≥</em>, then <em>T &lt; </em>4<em>R</em><sup>2</sup><em>/Œ≥</em><sup>2</sup>.</li>
</ol>
</li>
</ul>
In what follows, we will assume that k<em>w<sub>T</sub></em>k ‚â• 4<em>R</em><sup>2</sup><em>/Œ≥</em>.

<ol>
<li>Show that for any iteration <em>t </em>when mistake was made, the following holds:</li>
</ol>
k<em>w<sub>t</sub></em>k<sup>2 </sup>‚â§ (k<em>w<sub>t</sub></em><sub>‚àí1</sub>k + <em>Œ≥/</em>2)<sup>2 </sup>+ <em>R</em><sup>2</sup><em>.</em>

<ul>
<li>Infer from that that for any iteration <em>t</em>, we have</li>
</ul>
<em>.</em>

<ol>
<li>Using the previous question, show that for any iteration <em>t </em>such that either k<em>w<sub>t</sub></em><sub>‚àí1</sub>k ‚â•</li>
</ol>
, we have

<ol>
<li>Show that k<em>w</em><sub>0</sub>k ‚â§ <em>R </em>‚â§ 4<em>R</em><sup>2</sup><em>/Œ≥</em>. Since by assumption we have, conclude that there must exist some largest iteration <em>t</em><sub>0 </sub>such that k<em>w<sub>t</sub></em><sub>0</sub><sub>‚àí1</sub>k ‚â§ and. vi. Show that, and finally deduce the mistake bound.</li>
</ol>
<ul>
<li>[Making data linearly separable by feature space mapping] Consider the infinite dimensional feature space mapping</li>
</ul>
<em>.</em>

(It may be helpful to sketch the function for understanding the mapping and answering the questions below)

<ul>
<li>Show that for any <em>n </em>distinct points <em>x</em><sub>1</sub><em>,‚Ä¶,x<sub>n</sub></em>, there exists a <em>œÉ &gt; </em>0 such that the mapping Œ¶<em><sub>œÉ </sub></em>can linearly separate <em>any </em>binary labeling of the <em>n </em></li>
<li>Show that one can efficiently compute the dot products in this feature space, by giving an analytical formula for Œ¶<em><sub>œÉ</sub></em>(<em>x</em>) ¬∑ Œ¶<em><sub>œÉ</sub></em>(<em>x</em><sup>0</sup>) for arbitrary points <em>x </em>and <em>x</em><sup>0</sup>.</li>
</ul>
<ul>
<li>[Designing socially aware classifiers] Traditional Machine Learning research focuses on simply improving the accuracy. However, the model with the highest accuracy may be discriminatory and thus may have undesirable social impact that unintentionally hurts minority groups<a href="#_ftn1" name="_ftnref1"><sup>[1]</sup></a>. To overcome such undesirable impacts, researchers have put lots of effort in the field called Computational Fairness in recent years.</li>
</ul>
Two central problems of Computational Fairness are: (1) what is an appropriate definition of fairness that works under different settings of interest? (2) How can we achieve the proposed definitions without sacrificing on prediction accuracy?

In this problem, we will focus on some of the ways we can address the first problem. There are two categories of fairness definitions: individual fairness<a href="#_ftn2" name="_ftnref2"><sup>[2]</sup></a> and group fairness<a href="#_ftn3" name="_ftnref3"><sup>[3]</sup></a>. Most works in the literature focus on the group fairness. Here we will study some of the most popular group fairness definitions and explore them empirically on a real-world dataset.

Generally, group fairness concerns with ensuring that group-level statistics are same across all groups. A group is usually formed with respect to a feature called the sensitive attribute. Most common sensitive features include: gender, race, age, religion, income-level, etc. Thus, group fairness ensures that statistics across the sensitive attribute (such as across, say, different age groups) remain the same.

For simplicity, we only consider the setting of binary classification with a single sensitive attribute. Unless stated otherwise, we also consider the sensitive attribute to be binary. (Note that the binary assumption is only for convenience and results can be extended to non-binary cases as well.) Notations:

Denote <em>X </em>‚àà R<em><sup>d</sup></em><em>, A </em>‚àà {0<em>,</em>1} and <em>Y </em>‚àà {0<em>,</em>1} to be three random variables: non-sensitive features of an instance, the instance‚Äôs sensitive feature and the target label of the instance respectively, such that (<em>X,A,Y </em>) ‚àº D. Denote a classifier <em>f </em>: R<em><sup>d </sup></em>‚Üí {0<em>,</em>1} and denote <em>Y</em>ÀÜ := <em>f</em>(<em>X</em>).

For simplicity, we also use the following abbreviations:

P := P(<em>X,A,Y </em>)‚àº<em>D&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </em>and&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P<em>a </em>:= P(<em>X,a,Y </em>)‚àº<em>D</em>

We will explore the following are three fairness definitions.

<ul>
<li><em>Demographic Parity (DP)</em></li>
</ul>
<table width="356">
<tbody>
<tr>
<td width="293">P0[<em>Y</em>ÀÜ = <em>y</em>ÀÜ] = P1[<em>Y</em>ÀÜ = <em>y</em>ÀÜ]

(equal positive rate across the sensitive attribute)

<em>‚Äì Equalized Odds (EO)</em>
</td>
<td width="63">‚àÄ<em>y</em>ÀÜ ‚àà {0<em>,</em>1}</td>
</tr>
</tbody>
</table>
P0[<em>Y</em>ÀÜ = <em>y</em>ÀÜ | <em>Y </em>= <em>y</em>] = P1[<em>Y</em>ÀÜ = <em>y</em>ÀÜ | <em>Y </em>= <em>y</em>]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚àÄ<em>y, y</em>ÀÜ&nbsp;&nbsp; ‚àà {0<em>,</em>1}

(equal true positive- and true negative-rates across the sensitive attribute)

<ul>
<li><em>Predictive Parity (PP)</em></li>
</ul>
P0[<em>Y </em>= <em>y </em>| <em>Y</em>ÀÜ = <em>y</em>ÀÜ] = P1[<em>Y </em>= <em>y </em>| <em>Y</em>ÀÜ = <em>y</em>ÀÜ]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚àÄ<em>y, y</em>ÀÜ&nbsp;&nbsp; ‚àà {0<em>,</em>1}

(equal positive predictive- and negative predictive-value across the sensitive attribute)

Part 0: The basics.

<ul>
<li>Why is it not enough to just remove the sensitive attribute <em>A </em>from the dataset to achieve fairness as per the definitions above? Explain with a concrete example.</li>
</ul>
Part 1: Sometimes, people write the same fairness definition in different ways.

<ul>
<li>Show that the following two definitions for <em>Demographic Parity </em>is equivalent under our setting:</li>
</ul>
P0[<em>Y</em>ÀÜ = 1] = P1[<em>Y</em>ÀÜ = 1] ‚áê‚áí P[<em>Y</em>ÀÜ = 1] = P<em>a</em>[<em>Y</em>ÀÜ = 1]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚àÄ<em>a </em>‚àà {0<em>,</em>1}

Part 2: In this part, we will explore the COMPAS dataset (available in hw2data.zip). The task is to predict two year recidivism. Download the COMPAS dataset from the class‚Äôs website. In this dataset, the target label <em>Y </em>is twoyearrecid and the sensitive feature <em>A </em>is race.

<ul>
<li>Develop the following classifiers: (1) MLE based classifier, (2) nearest neighbor classifier, and (3) decision tree classifier, for the given dataset.</li>
</ul>
For MLE classifier, you can model the class conditional densities by a Multivariate Gaussian distribution. For nearest neighbor classifier, you should consider different values of <em>k </em>and the distance metric (e.g. <em>L</em><sub>1</sub><em>,L</em><sub>2</sub><em>,L</em><sub>‚àû</sub>).

(you may use builtin functions to develop your classifier, and you <em>do not </em>need to write the classifier form scratch.)

You <em>do not </em>need to submit any code on Courseworks.

<ul>
<li>Which classifier (discussed in previous part) is better for this prediction task? You must justify your answer with appropriate performance graphs demonstrating the superiority of one classifier over the other. Example things to consider: how does the training sample size affects the classification performance.</li>
<li>To what degree the fairness definitions are satisfied for each of the classifiers you developed? Show your results with appropriate performance graphs.</li>
</ul>
For each fairness measure, which classifier is the most fair? How would you summarize the difference of these algorithms?

<ul>
<li>Choose any one of the three fairness definitions. Describe a real-world scenario where this definition is most reasonable and applicable. What are the potential disadvantage(s) of this fairness definition?</li>
</ul>
(You are free to reference online and published materials to understand the strengths and weaknesses of each of the fairness definitions. Make sure cite all your resources.)

<a href="#_ftnref1" name="_ftn1">[1]</a> see e.g. Machine Bias by Angwin et al. for bias in recidivism predication, and Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification by Buolamwini and Gebru for bias in face recognition

<a href="#_ftnref2" name="_ftn2">[2]</a> see e.g. Fairness Through Awareness by Dwork et al.

<a href="#_ftnref3" name="_ftn3">[3]</a> see e.g. Equality of Opportunity in Supervised Learning by Hardt et al.
